{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For python2/3 compatibility\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def onehot_tensor(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], t.shape[1], num_classes))\n",
    "    for batch in range(t.shape[0]):\n",
    "        for row, col in enumerate(t[batch]):\n",
    "            out[batch, row, col] = 1\n",
    "    return out\n",
    "\n",
    "def onehot_vector(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for batch in range(t.shape[0]):\n",
    "        out[batch, t[batch]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional random fields\n",
    "\n",
    "# Definition\n",
    "\n",
    "We define the conditional random field for label sequence $y_1,\\ldots,y_n$:\n",
    "$$\n",
    "p(y|h) = \\frac{1}{Z} \\prod_i \\exp f(y_i,h_i) \\prod_{i=1}^{n-1} \\exp g(y_i,y_{i+1},h_i) \n",
    "$$\n",
    "The $h_i$ is the $i$th output of a recurrent (potentially bi-direncetional) RNN. $f(y_i,h_i)$ is the standard \n",
    "feed-forward pre-softmax output and $g(y_i,y_{i+1},h_i)$ is the random field interaction term. If we have $c$ \n",
    "classes we can implement $g(y_i,y_{i+1},h_i)$ as a linear output network with $c^2$ outputs.   \n",
    "\n",
    "The normalization constant is\n",
    "$$\n",
    "Z = \\sum_{y_1} \\ldots \\sum_{y_n} \\prod_i \\exp f(y_i,h_i) \\prod_{i=1}^{n-1} \\exp g(y_i,y_{i+1}h_i) \\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example \n",
    "\n",
    "Let us make a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50)\n",
      "(3, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "b = 3\n",
    "n = 50 # length of sequence\n",
    "c = 2 # number of classes\n",
    "\n",
    "g = np.zeros((b, n-1, c, c)) # independent variables\n",
    "f = np.ones((b, n, c)) # this should lead to equal probability for both classes \n",
    "\n",
    "# define some example data that we will use below to calculate the log likelihood\n",
    "y = np.zeros([b, n], dtype=np.int32) # all label belong to class 1\n",
    "y_hot = onehot_tensor(y, c)\n",
    "print(y.shape)\n",
    "print(y_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We can handle inference that is computing $Z$, marginals\n",
    "$$\n",
    "p(y_i|h) = \\frac{1}{Z} \\sum_{y_1} \\ldots \\sum_{y_{i-1}} \\sum_{y_{i+1}} \\ldots \\sum_{y_n} \\prod_i \\exp f(y_i,h_i) \\prod_{i=1}^{n-1} \\exp g(y_i,y_{i+1},h_i) \n",
    "$$\n",
    "and the most probably output sequence\n",
    "$$\n",
    "{\\rm argmax}_y p(y|h)\n",
    "$$\n",
    "by dynamic programming. The dynamic programming requires one forward and one backward pass to solve the two first tasks and a different forward and backward pass (the so-called Viterbi algorithm) for the last task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward recursion\n",
    "\n",
    "See Bishop 8.4.1 for justification. Define $\\mu_\\alpha(y_i)$ (a vector of dimensionality the number of classes $c$) as the following recursion\n",
    "\\begin{align*}\n",
    "\\mu_\\alpha(y_1) & = \\exp f(y_1,h_1) \\\\\n",
    "\\mu_\\alpha(y_i) & = \\exp f(y_i,h_i) \\sum_{y_{i-1}} \\exp g(y_{i-1},y_{i},h_{i-1}) \\mu_\\alpha(y_{i-1})\n",
    "\\end{align*}\n",
    "In practice the sum will be a $c$-dimensional matrix multiplication.\n",
    "In the log-domain $\\nu_\\alpha(y_i) \\equiv \\log \\mu_\\alpha(y_i)$ we have\n",
    "\\begin{align*}\n",
    "\\nu_\\alpha(y_1) & = f(y_1,h_1) \\\\\n",
    "\\nu_\\alpha(y_i) & = f(y_i,h_i) + \\log \\sum_{y_{i-1}} \\exp ( g(y_{i-1},y_{i},h_{i-1}) + \\nu_\\alpha(y_{i-1}) ) \\ . \n",
    "\\end{align*}\n",
    "We might need a bit of max book-keeping to make this stable but let us forget that in the first implementation. \n",
    "We can probably handle this by introducing a ${\\rm log\\_sum\\_exp}$ function: ${\\rm log\\_sum\\_exp}(x)=x_{{\\rm max}} + \\log \\sum_k \\exp(x_m - x_{{\\rm max}}$, where $x_{{\\rm max}} =\\max_k x_k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50, 2)\n",
      "[[  1.           2.69314718   4.38629436   6.07944154   7.77258872\n",
      "    9.4657359   11.15888308  12.85203026  14.54517744  16.23832463\n",
      "   17.93147181  19.62461899  21.31776617  23.01091335  24.70406053\n",
      "   26.39720771  28.09035489  29.78350207  31.47664925  33.16979643\n",
      "   34.86294361  36.55609079  38.24923797  39.94238515  41.63553233\n",
      "   43.32867951  45.02182669  46.71497388  48.40812106  50.10126824\n",
      "   51.79441542  53.4875626   55.18070978  56.87385696  58.56700414\n",
      "   60.26015132  61.9532985   63.64644568  65.33959286  67.03274004\n",
      "   68.72588722  70.4190344   72.11218158  73.80532876  75.49847594\n",
      "   77.19162313  78.88477031  80.57791749  82.27106467  83.96421185]\n",
      " [  1.           2.69314718   4.38629436   6.07944154   7.77258872\n",
      "    9.4657359   11.15888308  12.85203026  14.54517744  16.23832463\n",
      "   17.93147181  19.62461899  21.31776617  23.01091335  24.70406053\n",
      "   26.39720771  28.09035489  29.78350207  31.47664925  33.16979643\n",
      "   34.86294361  36.55609079  38.24923797  39.94238515  41.63553233\n",
      "   43.32867951  45.02182669  46.71497388  48.40812106  50.10126824\n",
      "   51.79441542  53.4875626   55.18070978  56.87385696  58.56700414\n",
      "   60.26015132  61.9532985   63.64644568  65.33959286  67.03274004\n",
      "   68.72588722  70.4190344   72.11218158  73.80532876  75.49847594\n",
      "   77.19162313  78.88477031  80.57791749  82.27106467  83.96421185]]\n"
     ]
    }
   ],
   "source": [
    "def log_sum_exp(z, axis=1):\n",
    "    zmax = np.max(z, axis)\n",
    "    return zmax + np.log( np.sum( np.exp( z - np.expand_dims(zmax, axis)), axis))\n",
    "     \n",
    "def forward_step(nu, f, g, class_dimension):\n",
    "    return f + log_sum_exp(g + np.expand_dims(nu, class_dimension+1), axis=class_dimension)\n",
    "\n",
    "def forward_pass(f, g, time_major=False):\n",
    "    if not time_major:\n",
    "        f = np.transpose(f, [1, 0, 2])\n",
    "        g = np.transpose(g, [1, 0, 2, 3])\n",
    "    nu = np.zeros(np.shape(f))\n",
    "    nu[0] = f[0]\n",
    "    sequence_length = f.shape[0]\n",
    "    class_dimension = 1\n",
    "    for index in range(1, sequence_length):\n",
    "        fs = forward_step(nu[index-1], f[index], g[index-1], class_dimension)\n",
    "        nu[index] = fs\n",
    "    if not time_major:\n",
    "        nu = np.transpose(nu, [1, 0, 2])\n",
    "    return nu\n",
    "\n",
    "nu_alp = forward_pass(f, g)\n",
    "print(nu_alp.shape)\n",
    "print(nu_alp[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward recursion\n",
    "\n",
    "The backward recursion follows the same reasoning (details omitted): \n",
    "\\begin{align*}\n",
    "\\nu_\\beta(y_n) & = 0 \\\\\n",
    "\\nu_\\beta(y_i) & = \\log \\sum_{y_{i+1}} \\exp ( f(y_{i+1},h_{i+1}) + g(y_{i},y_{i+1},h_{i}) + \\nu_\\alpha(y_{i+1}) ) \\ . \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 82.96421185  81.27106467  79.57791749  77.88477031  76.19162313\n",
      "   74.49847594  72.80532876  71.11218158  69.4190344   67.72588722\n",
      "   66.03274004  64.33959286  62.64644568  60.9532985   59.26015132\n",
      "   57.56700414  55.87385696  54.18070978  52.4875626   50.79441542\n",
      "   49.10126824  47.40812106  45.71497388  44.02182669  42.32867951\n",
      "   40.63553233  38.94238515  37.24923797  35.55609079  33.86294361\n",
      "   32.16979643  30.47664925  28.78350207  27.09035489  25.39720771\n",
      "   23.70406053  22.01091335  20.31776617  18.62461899  16.93147181\n",
      "   15.23832463  13.54517744  11.85203026  10.15888308   8.4657359\n",
      "    6.77258872   5.07944154   3.38629436   1.69314718   0.        ]\n",
      " [ 82.96421185  81.27106467  79.57791749  77.88477031  76.19162313\n",
      "   74.49847594  72.80532876  71.11218158  69.4190344   67.72588722\n",
      "   66.03274004  64.33959286  62.64644568  60.9532985   59.26015132\n",
      "   57.56700414  55.87385696  54.18070978  52.4875626   50.79441542\n",
      "   49.10126824  47.40812106  45.71497388  44.02182669  42.32867951\n",
      "   40.63553233  38.94238515  37.24923797  35.55609079  33.86294361\n",
      "   32.16979643  30.47664925  28.78350207  27.09035489  25.39720771\n",
      "   23.70406053  22.01091335  20.31776617  18.62461899  16.93147181\n",
      "   15.23832463  13.54517744  11.85203026  10.15888308   8.4657359\n",
      "    6.77258872   5.07944154   3.38629436   1.69314718   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def backward_step(nu, f, g, class_dimension):\n",
    "    return log_sum_exp(np.expand_dims(f + nu, class_dimension) + g, axis=class_dimension+1)\n",
    "\n",
    "def backward_pass(f, g, time_major=False):\n",
    "    if not time_major:\n",
    "        f = np.transpose(f, [1, 0, 2])\n",
    "        g = np.transpose(g, [1, 0, 2, 3])\n",
    "    nu = np.zeros(np.shape(f))\n",
    "    sequence_length = f.shape[0]\n",
    "    class_dimension = 1\n",
    "    for index in range(1, sequence_length):\n",
    "        nu[-index-1] = backward_step(nu[-index], f[-index],g[-index],\n",
    "                                     class_dimension=class_dimension)\n",
    "    if not time_major:\n",
    "        nu = np.transpose(nu, [1, 0, 2])\n",
    "    return nu\n",
    "\n",
    "nu_bet = backward_pass( f, g)\n",
    "\n",
    "print(nu_bet[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log likelihood\n",
    "\n",
    "In training we have a sequence $y$ and we want to evaluate its probability. We have\n",
    "$$\n",
    "\\log p(y|h) = \\sum_i f(y_i,h_i) + \\sum_{i=1}^{n-1}  g(y_i,y_{i+1},h_i) - \\log Z\n",
    "$$\n",
    "Now the miracle happens because we can write $Z$ as\n",
    "$$\n",
    "Z = \\sum_{y_i} \\exp(\\nu_\\alpha(y_i) + \\nu_\\beta(y_i) )\n",
    "$$\n",
    "(This result holds for any choice of $i\\in \\{1,\\ldots,n\\}$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood\n",
      "-34.657359028\n",
      "\n",
      "log_likelihood_vectorized\n",
      "-34.657359028\n",
      "\n",
      "-34.657359028\n",
      "[ 84.65735903  84.65735903  84.65735903]\n",
      "[ 84.65735903  84.65735903  84.65735903]\n",
      "[ 84.65735903  84.65735903  84.65735903]\n"
     ]
    }
   ],
   "source": [
    "def logZ(nu_alp, nu_bet, index=0, time_major=False):\n",
    "    if not time_major:\n",
    "        nu_alp = np.transpose(nu_alp, [1, 0, 2])\n",
    "        nu_bet = np.transpose(nu_bet, [1, 0, 2])\n",
    "    class_dimension = 1\n",
    "    return log_sum_exp(nu_alp[index]+nu_bet[index],\n",
    "                       axis=class_dimension) \n",
    "\n",
    "def log_likelihood(y, f, g, nu_alp, nu_bet, mean_batch=True, time_major=False):\n",
    "    if not time_major:\n",
    "        y = np.transpose(y, [1, 0])\n",
    "        f = np.transpose(f, [1, 0, 2])\n",
    "        g = np.transpose(g, [1, 0, 2, 3])\n",
    "    sequence_length = np.shape(f)[0]\n",
    "    batch_size = np.shape(f)[1]\n",
    "    f_term = np.zeros((batch_size))\n",
    "    g_term = np.zeros((batch_size))\n",
    "    z_term = np.zeros((batch_size))\n",
    "    for index in range(sequence_length):\n",
    "        f_term += f[index, np.arange(batch_size), y[index]] # f(y_i,h_i) term\n",
    "    for index in range(sequence_length - 1):\n",
    "        g_term += g[index, np.arange(batch_size), y[index + 1], y[index]] # g(y_i,y_i+1,h_i)\n",
    "    z_term = logZ( nu_alp, nu_bet)\n",
    "    log_like = f_term + g_term - z_term\n",
    "    if mean_batch:\n",
    "        log_like = np.mean(log_like)\n",
    "    return log_like\n",
    "\n",
    "def extract_values_vector_f(f, y):\n",
    "    dim0, dim1, dim2 = f.shape\n",
    "    idx0 = np.repeat(np.arange(dim0), dim1)\n",
    "    idx1 = np.asarray([np.arange(dim1)] * dim0).reshape([-1])\n",
    "    idx2 = y.reshape([-1])\n",
    "    return f[idx0, idx1, idx2].reshape([dim0, dim1])\n",
    "\n",
    "def extract_values_vector_g(g, y):\n",
    "    g_seq_dim, batch_dim, c_dim1, c_dim2 = g.shape\n",
    "    g_seq_idx = np.repeat(np.arange(g_seq_dim), batch_dim)\n",
    "    g_batch_idx = np.asarray([np.arange(batch_dim)] * (g_seq_dim)).reshape([-1])\n",
    "    y1 = y[g_seq_idx, g_batch_idx]\n",
    "    y2 = y[g_seq_idx+1, g_batch_idx]\n",
    "    john = g[g_seq_idx, g_batch_idx, y1, y2].reshape([g_seq_dim, batch_dim])\n",
    "    return g[g_seq_idx, g_batch_idx, y1, y2].reshape([g_seq_dim, batch_dim])\n",
    "\n",
    "def log_likelihood_vectorized(y, f, g, nu_alp, nu_bet, mean_batch=True, time_major=False):\n",
    "    if not time_major:\n",
    "        y = np.transpose(y, [1, 0])\n",
    "        f = np.transpose(f, [1, 0, 2])\n",
    "        g = np.transpose(g, [1, 0, 2, 3])\n",
    "    f_term = np.sum(extract_values_vector_f(f, y), axis=0)\n",
    "    g_term = np.sum(extract_values_vector_g(g, y), axis=0)\n",
    "    z_term = logZ(nu_alp, nu_bet)\n",
    "    log_like = f_term + g_term - z_term\n",
    "    if mean_batch:\n",
    "        log_like = np.mean(log_like)\n",
    "    return log_like\n",
    "\n",
    "print(\"log_likelihood\")\n",
    "print(log_likelihood(y, f, g, nu_alp, nu_bet))\n",
    "print()\n",
    "print(\"log_likelihood_vectorized\")\n",
    "print(log_likelihood_vectorized(y, f, g, nu_alp, nu_bet))\n",
    "print()\n",
    "print(n*np.log(0.5)) # compare to log likelihood for n independent variables with 0.5 probability\n",
    "\n",
    "print(logZ( nu_alp, nu_bet)) # should give same log Z no matter what slice we use\n",
    "print(logZ( nu_alp, nu_bet, 2))\n",
    "print(logZ( nu_alp, nu_bet, n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood\n",
      "-34.657359028\n"
     ]
    }
   ],
   "source": [
    "# The one-hot encoded way of calculating log_likelihood\n",
    "\n",
    "def log_likelihood_hot(y, f, g, nu_alp, nu_bet, mean_batch=True, time_major=False):\n",
    "    if time_major:\n",
    "        y = np.transpose(y, [1, 0, 2])\n",
    "        f = np.transpose(f, [1, 0, 2])\n",
    "        g = np.transpose(g, [1, 0, 2, 3])\n",
    "    f_term = np.sum(f*y, axis=(1, 2))\n",
    "    y_i = np.expand_dims(y[:, :-1], axis=3)\n",
    "    y_plus = np.expand_dims(y[:, 1:], axis=2)\n",
    "    g_term = np.sum(g*y_i*y_plus, axis=(1,2,3))\n",
    "    z_term = logZ(nu_alp, nu_bet)\n",
    "    log_like = f_term + g_term - z_term\n",
    "    if mean_batch:\n",
    "        log_like = np.mean(log_like)\n",
    "    return log_like\n",
    "    \n",
    "print(\"log_likelihood\")\n",
    "print(log_likelihood_hot(y_hot, f, g, nu_alp, nu_bet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We will usually choose as out prediction the most probable label so we need to compute the marginal. Again the miracle (called dynamic programming) happens:\n",
    "$$\n",
    "p(y_i|h) = \\frac{1}{Z} \\exp(\\nu_\\alpha(y_i) + \\nu_\\beta(y_i) ) \\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50, 2)\n",
      "(3, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "print(nu_alp.shape)\n",
    "print(nu_bet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      " [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]]\n",
      "[ 0.5  0.5]\n",
      "[ 0.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "def log_marginal(nu_alp, nu_bet, index=None, time_major=False):\n",
    "    if not time_major:\n",
    "        nu_alp = np.transpose(nu_alp, [1, 0, 2])\n",
    "        nu_bet = np.transpose(nu_bet, [1, 0, 2])\n",
    "    sequence_length = nu_alp.shape[0]\n",
    "    if index is None:\n",
    "        index=np.arange(sequence_length)\n",
    "    r1 = nu_alp[index]\n",
    "    r2 = nu_bet[index]\n",
    "    r3 = np.expand_dims(logZ(nu_alp, nu_bet, time_major=True), axis=1)\n",
    "    res = r1 + r2 - r3\n",
    "    if not time_major:\n",
    "        if len(res.shape) == 3:\n",
    "            res = np.transpose(res, [1, 0, 2])\n",
    "    return res\n",
    "\n",
    "print(np.exp(log_marginal( nu_alp, nu_bet))[0].T)\n",
    "print(np.exp(log_marginal( nu_alp, nu_bet, 0))[0].T)\n",
    "print(np.exp(log_marginal( nu_alp, nu_bet, 2))[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi decoding (max-sum algorithm)\n",
    "This is useful when we are interested in the most probable sequence and not just the most probable classification at each position \n",
    "(think machine translation). This derivation is an adaptation of https://en.wikipedia.org/wiki/Viterbi_algorithm.  As a warm-up calculate\n",
    "$$\n",
    "\\max_y \\log p(y|h) = \\max_{y_1}\\cdots \\max_{y_n} \\left\\{ \\sum_i f(y_i,h_i) + \\sum_{i=1}^{n-1}  g(y_i,y_{i+1},h_i) - \\log Z \\right\\} \\ .\n",
    "$$\n",
    "We can solve this with dynamic programming again by introducing\n",
    "\\begin{align*}\n",
    "\\mu(y_{i+1}) & = \\max_{y_i}  \\left\\{ f(y_i,h_i) + g(y_i,y_{i+1},h_i) + \\mu(y_{i}) \\right\\} \\\\\n",
    "\\mu(y_{1}) & = 0 \\ .\n",
    "\\end{align*}\n",
    "Now we simply get\n",
    "$$\n",
    "\\max_y \\log p(y|h) = \\max_{y_n} \\mu(y_{n}) - \\log Z \\ . \n",
    "$$\n",
    "To prove this start from $\\max_{y_1}$ in the original expression and introduce $\\mu(y_2)$ and the remaining $\\mu$s recursively. \n",
    "\n",
    "In order to find the most probable sequence we have to introduce a bit more bookkeeping. We immediately see that the the last symbol in the most probable sequence is ${\\rm argmax}_{y_n} \\mu(y_{n})$. But to get the second last symbol and so on requires that we keep track of the max value sequences for all possible configurations of $y_n$. We therefore introduce a variable that keeps the index of proceeding symbol for each setting of the $i+1$ symbol:\n",
    "\\begin{align*}\n",
    "\\nu_{i+1}(y_{i+1}) & = {\\rm argmax}_{y_i} \\left\\{ g(y_i,y_{i+1},h_i) + \\mu(y_{i}) \\right\\}\n",
    "\\end{align*}\n",
    "Now we have the information for find the most probable sequence $s_1,\\ldots,s_n$ from a backward recursion, $i=n-1,n-2,\\ldots,1$:\n",
    "\\begin{align*}\n",
    "s_n & = {\\rm argmax}_{y_n} \\mu(y_{n})\\\\ \n",
    "s_i & = \\nu_{i+1}(y_{i+1}=s_{i+1}) \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def forward_step_max(nu, f, g, class_dimension):\n",
    "    return f + np.max(g + np.expand_dims(nu, class_dimension+1), axis=class_dimension)\n",
    "\n",
    "def viterbi( f, g, time_major=False):\n",
    "    if not time_major:\n",
    "        f = np.transpose(f, [1, 0, 2])\n",
    "        g = np.transpose(g, [1, 0, 2, 3])\n",
    "    sequence_length = np.shape(f)[0]\n",
    "    class_dimension = 1\n",
    "    nu = np.zeros(f.shape)\n",
    "    nu_label = np.zeros(f.shape, dtype=np.int32)\n",
    "    viterbi_seq = np.zeros(np.shape(f[:,:,0]), dtype=np.int32)\n",
    "    nu[0] = f[0]\n",
    "    for index in range(1, sequence_length):\n",
    "        nu[index] = forward_step_max(nu[index-1], f[index], g[index-1], class_dimension)\n",
    "        nu_label[index] = np.argmax(g[index-1] + np.expand_dims(nu[index-1], class_dimension+1), axis=class_dimension)\n",
    "    viterbi_seq[-1] = np.argmax(nu[-1], axis=class_dimension)\n",
    "    for index in range(1, sequence_length):\n",
    "        #viterbi_seq[-index-1] = nu_label[-index,np.arange(f.shape[1]),viterbi_seq[-index]]\n",
    "        viterbi_seq[-index-1] = np.sum( nu_label[-index] * onehot_vector(viterbi_seq[-index], np.shape(f)[2]), axis=1)\n",
    "    if not time_major:\n",
    "        viterbi_seq = np.transpose(viterbi_seq, [1, 0])\n",
    "    # print(np.transpose(nu, [1, 0, 2]))\n",
    "    return viterbi_seq #, np.max(nu[-1], axis=class_dimension)\n",
    "        \n",
    "print(viterbi( f, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "We will in some cases want to draw samples from $p(y|h)$. It could be that we wanted to generate alternative translations in a machine translation model or let $y$ be discrete latent variables in a generative model. We can write the joint using the chain rule \n",
    "$$\n",
    "p(y|h) = p(y_1|y_2,\\ldots,y_n,h)p(y_2|y_3,\\ldots,y_n,h) \\cdots p(y_n|h)\n",
    "$$\n",
    "and then sample the joint by sampling one term at a time starting from the last. The $p(y_n|h)$ we can sample from using the result for the forward pass: $p(y_n|h) \\propto \\exp{\\mu_\\alpha(y_n)}$ . To get the subsequent terms we observe that due to the chain structure $y_{i+1}$ ``blocks'' the dependence of the remaining variables: \n",
    "$$\n",
    "p(y_i|y_{i+1},\\ldots,y_n,h)=p(y_i|y_{i+1},h) \\ .\n",
    "$$\n",
    "The conditional is always proportional to the joint:\n",
    "$$\n",
    "p(y_i|y_{i+1},h) \\propto p(y_i,y_{i+1}|h) = \\frac{1}{Z} \n",
    "\\sum_{y_1} \\ldots \\sum_{y_{i-1}} \\sum_{y_{i+1}} \\ldots \\sum_{y_n} \n",
    "\\prod_i \\exp f(y_i,h_i) \\prod_{i=1}^{n-1} \\exp g(y_i,y_{i+1},h_i) \n",
    "$$\n",
    "Since $y_{i+1}$ is observed we can ignore dependence on $y_{i+2}$ and onwards. The marginalization over $y_1, \\ldots, y_{i-1}$ is corresponds exactly to the computation in $\\mu_\\alpha$ so we get\n",
    "\\begin{align*}\n",
    "p(y_i|y_{i+1},\\ldots,y_n,h) & \\propto \\exp \\mu_\\alpha(y_i) + g(y_i,y_{i+1},h_i)   \\\\\n",
    "p(y_n|h) & \\propto \\exp{\\mu_\\alpha(y_n)} \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "def multinomial_vectorized(logp, axis=1):\n",
    "    sample = np.zeros(logp.shape, dtype=np.int32)\n",
    "    p = np.exp( logp - np.expand_dims(log_sum_exp( logp, axis=axis), axis) )\n",
    "    for b in np.arange(p.shape[0]):\n",
    "        sample[b] = np.random.multinomial(1, p[b])\n",
    "    return sample\n",
    "\n",
    "def sampling_ffbs_hot( f, g, time_major=False):\n",
    "    nu_alp = forward_pass( f, g)\n",
    "    if not time_major:\n",
    "        nu_alp = np.transpose(nu_alp, [1, 0, 2])\n",
    "        g = np.transpose(g, [1, 0, 2, 3])\n",
    "    sample = np.zeros(np.shape(nu_alp), dtype=np.int32)\n",
    "    sample[-1] = multinomial_vectorized(nu_alp[-1]) \n",
    "    sequence_length = nu_alp.shape[0]\n",
    "    for index in range(1, sequence_length):\n",
    "        sample[-index-1] = multinomial_vectorized( \n",
    "            np.sum( g[-index]*np.expand_dims(sample[-index], axis=2), axis=2) + nu_alp[-index])\n",
    "    if not time_major:\n",
    "        sample = np.transpose(sample, [1, 0, 2])\n",
    "    return sample\n",
    "\n",
    "print(sampling_ffbs_hot( f, g)[0,0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More examples\n",
    "\n",
    "## Example 2 \n",
    "\n",
    "Still independent variables but now with a bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nu_alp\n",
      "[[  3.  12.  48.]\n",
      " [  1.   4.  16.]]\n",
      "\n",
      "nu_bet\n",
      "[[ 16.   4.   1.]\n",
      " [ 16.   4.   1.]]\n",
      "\n",
      "logZ\n",
      "4.15888308336\n",
      "4.15888308336\n",
      "4.15888308336\n",
      "4.15888308336\n",
      "\n",
      "log_likelihood\n",
      "-0.863046217355\n",
      "\n",
      "log_likelihood_vectorized\n",
      "-0.863046217355\n",
      "\n",
      "compare to above\n",
      "-0.863046217355\n",
      "\n",
      "log_marginal\n",
      "[[ 0.75  0.75  0.75]\n",
      " [ 0.25  0.25  0.25]]\n"
     ]
    }
   ],
   "source": [
    "b_2 = 4 # batch_size\n",
    "n_2 = 3 # length of sequence\n",
    "c_2 = 2 # number of classes\n",
    "\n",
    "g_2 = np.zeros((b_2, n_2-1, c_2, c_2)) # independent variables\n",
    "f_2 = np.zeros((b_2, n_2, c_2)) #\n",
    "\n",
    "f_2[:, :, 0] = np.log(3) \n",
    "f_2[:, :, 1] = np.log(1) # this should give probability 3 / (3 + 1 ) = 3/4 for class 1\n",
    "\n",
    "# define some example data that we will use below to calculate log likelihood\n",
    "y_2 = np.zeros([b_2, n_2], dtype=np.int32) # all label belong to class 1\n",
    "\n",
    "nu_alp_2 = forward_pass(f_2, g_2)\n",
    "print(\"nu_alp\")\n",
    "print(np.exp(nu_alp_2)[0].T)\n",
    "print()\n",
    "\n",
    "nu_bet_2 = backward_pass(f_2, g_2)\n",
    "print(\"nu_bet\")\n",
    "print(np.exp(nu_bet_2)[0].T)\n",
    "print()\n",
    "\n",
    "print(\"logZ\")\n",
    "print(logZ(nu_alp_2, nu_bet_2)[0]) # should give same log Z no matter what slice we use\n",
    "print(logZ(nu_alp_2, nu_bet_2, 2)[0])\n",
    "print(logZ(nu_alp_2, nu_bet_2, n_2-1)[0])\n",
    "print(n_2*np.log(4)) # exact log Z\n",
    "print()\n",
    "\n",
    "print(\"log_likelihood\")\n",
    "print(log_likelihood(y_2, f_2, g_2, nu_alp_2, nu_bet_2))\n",
    "print()\n",
    "print(\"log_likelihood_vectorized\")\n",
    "print(log_likelihood_vectorized(y_2, f_2, g_2, nu_alp_2, nu_bet_2))\n",
    "print()\n",
    "print(\"compare to above\")\n",
    "print(n_2*np.log(3.0/4.0)) # compare to independent\n",
    "print()\n",
    "print(\"log_marginal\")\n",
    "print(np.exp(log_marginal(nu_alp_2, nu_bet_2))[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 \n",
    "    \n",
    "Now with dependent variables that will make class 1 more likely at each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 1.  0.]\n",
      " [ 0.  0.]]\n",
      "\n",
      "nu_alp\n",
      "[[   3.           27.46453646  235.96905113]\n",
      " [   1.            4.           31.46453646]]\n",
      "\n",
      "nu_bet\n",
      "[[ 78.65635038   9.15484549   1.        ]\n",
      " [ 31.46453646   4.           1.        ]]\n",
      "\n",
      "logZ\n",
      "5.5888712648\n",
      "5.5888712648\n",
      "5.5888712648\n",
      "\n",
      "compare\n",
      "5.5888712648\n",
      "\n",
      "log_likelihood\n",
      "-0.293034398791\n",
      "\n",
      "log_likelihood_vectorized\n",
      "-0.293034398791\n",
      "\n",
      "compare\n",
      "-0.863046217355\n",
      "\n",
      "exp log_marginal\n",
      "[[ 0.88234635  0.94017206  0.88234635]\n",
      " [ 0.11765365  0.05982794  0.11765365]]\n",
      "\n",
      "Samples\n",
      "[[[1 0]\n",
      "  [1 0]\n",
      "  [1 0]]\n",
      "\n",
      " [[1 0]\n",
      "  [1 0]\n",
      "  [1 0]]\n",
      "\n",
      " [[1 0]\n",
      "  [1 0]\n",
      "  [1 0]]\n",
      "\n",
      " [[1 0]\n",
      "  [1 0]\n",
      "  [1 0]]]\n"
     ]
    }
   ],
   "source": [
    "b_3 = 4 # batch_size\n",
    "n_3 = 3 # length of sequence\n",
    "c_3 = 2 # number of classes\n",
    "\n",
    "g_3 = np.zeros((b_3, n_3-1, c_3, c_3)) # independent variables\n",
    "f_3 = np.zeros((b_3, n_3, c_3)) #\n",
    "\n",
    "f_3[:, :, 0] = np.log(3) \n",
    "f_3[:, :, 1] = np.log(1) # this should give probability 3 / (3 + 1 ) = 3/4 for class 1\n",
    "\n",
    "# define some example data that we will use below to calculate log likelihood\n",
    "y_3 = np.zeros([b_3, n_3], dtype=np.int32) # all label belong to class 1\n",
    "\n",
    "g_3[:, :, 0, 0] = np.ones([n_3-1])\n",
    "\n",
    "print(g_3[0, 0])\n",
    "print(g_3[0, 1])\n",
    "print()\n",
    "\n",
    "nu_alp_3 = forward_pass(f_3, g_3)\n",
    "nu_bet_3 = backward_pass(f_3, g_3)\n",
    "\n",
    "print(\"nu_alp\")\n",
    "print(np.exp(nu_alp_3)[0].T)\n",
    "print()\n",
    "print(\"nu_bet\")\n",
    "print(np.exp(nu_bet_3)[0].T)\n",
    "print()\n",
    "print(\"logZ\")\n",
    "print(logZ(nu_alp_3, nu_bet_3)[0]) # should give same log Z no matter what slice we use\n",
    "print(logZ(nu_alp_3, nu_bet_3, 1)[0])\n",
    "print(logZ(nu_alp_3, nu_bet_3, n_3-1)[0])\n",
    "print()\n",
    "print(\"compare\")\n",
    "print(np.log(np.power(3,n_3)*np.power(np.exp(1),n_3-1)+2*np.power(3,n_3-1)*np.power(np.exp(1),n_3-2)+np.power(3,n_3-1)+3*np.power(3,n_3-2)+1)) # exact log Z\n",
    "print()\n",
    "print(\"log_likelihood\")\n",
    "print(log_likelihood(y_3, f_3, g_3, nu_alp_3, nu_bet_3))\n",
    "print()\n",
    "print(\"log_likelihood_vectorized\")\n",
    "print(log_likelihood_vectorized(y_3, f_3, g_3, nu_alp_3, nu_bet_3))\n",
    "print()\n",
    "print(\"compare\")\n",
    "print(n_3*np.log(3.0/4.0)) # compare to independent\n",
    "print()\n",
    "print(\"exp log_marginal\")\n",
    "print(np.exp(log_marginal(nu_alp_3, nu_bet_3))[0].T)\n",
    "print()\n",
    "print(\"Samples\")\n",
    "print(sampling_ffbs_hot(f_3, g_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4\n",
    "\n",
    "Anto-correlated dependent variables and likelihood evaluated for different sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0.]\n",
      " [ 0.  0.]]\n",
      "[[-1.  0.]\n",
      " [ 0.  0.]]\n",
      "\n",
      "nu_alp\n",
      "[[  3.           6.31091497  18.96496762]\n",
      " [  1.           4.          10.31091497]]\n",
      "\n",
      "nu_bet\n",
      "[[  6.32165587   2.10363832   1.        ]\n",
      " [ 10.31091497   4.           1.        ]]\n",
      "\n",
      "logZ\n",
      "3.37676405723\n",
      "3.37676405723\n",
      "3.37676405723\n",
      "\n",
      "\n",
      "log_likelihood\n",
      "-2.22884562422\n",
      "\n",
      "log_likelihood_vectorized\n",
      "-2.22884562422\n",
      "\n",
      "log_likelihood_hot\n",
      "-2.22884562422\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.12481443  0.1130936   0.30742028  0.10247343  0.1130936   0.10247343\n",
      "  0.10247343  0.03415781]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "Viterbi\n",
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "exp log_marginal\n",
      "[[ 0.64780174  0.45347506  0.64780174]\n",
      " [ 0.35219826  0.54652494  0.35219826]]\n",
      "\n",
      "Samples\n",
      "[[[0 1]\n",
      "  [0 1]\n",
      "  [1 0]]\n",
      "\n",
      " [[0 1]\n",
      "  [1 0]\n",
      "  [1 0]]]\n"
     ]
    }
   ],
   "source": [
    "b_4 = 8 # batch_size\n",
    "n_4 = 3 # length of sequence\n",
    "c_4 = 2 # number of classes\n",
    "\n",
    "g_4 = np.zeros((b_4, n_4-1, c_4, c_4)) # independent variables\n",
    "f_4 = np.zeros((b_4, n_4, c_4)) #\n",
    "\n",
    "g_4[:, :, 0, 0] = -np.ones([n_4-1]) # no longer independent variables\n",
    "\n",
    "print(g_4[0, 0])\n",
    "print(g_4[0, 1])\n",
    "print()\n",
    "\n",
    "f_4[:, :, 0] = np.log(3) \n",
    "f_4[:, :, 1] = np.log(1) # this should give probability 3 / (3 + 1 ) = 3/4 for class 1\n",
    "\n",
    "# define some example data that we will use below to calculate log likelihood\n",
    "y_4 = np.zeros([b_4, n_4], dtype=np.int32) # all label belong to class 1\n",
    "y_4[1, :] = [0, 0, 1]\n",
    "y_4[2, :] = [0, 1, 0]\n",
    "y_4[3, :] = [0, 1, 1]\n",
    "y_4[4, :] = [1, 0, 0]\n",
    "y_4[5, :] = [1, 0, 1]\n",
    "y_4[6, :] = [1, 1, 0]\n",
    "y_4[7, :] = [1, 1, 1]\n",
    "\n",
    "y_4_hot = onehot_tensor(y_4, 2)\n",
    "\n",
    "nu_alp_4 = forward_pass(f_4, g_4)\n",
    "nu_bet_4 = backward_pass(f_4, g_4)\n",
    "\n",
    "print(\"nu_alp\")\n",
    "print(np.exp(nu_alp_4)[0].T)\n",
    "print()\n",
    "print(\"nu_bet\")\n",
    "print(np.exp(nu_bet_4)[0].T)\n",
    "print()\n",
    "print(\"logZ\")\n",
    "print(logZ(nu_alp_4, nu_bet_4)[0]) # should give same log Z no matter what slice we use\n",
    "print(logZ(nu_alp_4, nu_bet_4, 1)[0])\n",
    "print(logZ(nu_alp_4, nu_bet_4, n_4-1)[0])\n",
    "print()\n",
    "#print(\"compare\")\n",
    "#print(np.log(np.power(3,n_3)*np.power(np.exp(1),n_3-1)+2*np.power(3,n_3-1)*np.power(np.exp(1),n_3-2)+np.power(3,n_3-1)+3*np.power(3,n_3-2)+1)) # exact log Z\n",
    "print()\n",
    "print(\"log_likelihood\")\n",
    "print(log_likelihood(y_4, f_4, g_4, nu_alp_4, nu_bet_4))\n",
    "print()\n",
    "print(\"log_likelihood_vectorized\")\n",
    "print(log_likelihood_vectorized(y_4, f_4, g_4, nu_alp_4, nu_bet_4))\n",
    "print()\n",
    "print(\"log_likelihood_hot\")\n",
    "print(log_likelihood_hot(y_4_hot, f_4, g_4, nu_alp_4, nu_bet_4))\n",
    "print()\n",
    "print(\"the probability for each possible sequence\")\n",
    "print(np.exp(log_likelihood_hot(y_4_hot, f_4, g_4, nu_alp_4, nu_bet_4, mean_batch=False)))\n",
    "print()\n",
    "print(\"check that the probability add up to one\")\n",
    "print(np.sum(np.exp(log_likelihood_hot(y_4_hot, f_4, g_4, nu_alp_4, nu_bet_4, mean_batch=False))))\n",
    "print()\n",
    "print(\"Viterbi\")\n",
    "print(viterbi(f_4, g_4))\n",
    "print()\n",
    "print(\"exp log_marginal\")\n",
    "print(np.exp(log_marginal(nu_alp_4, nu_bet_4))[0].T)\n",
    "print()\n",
    "print(\"Samples\")\n",
    "print(sampling_ffbs_hot(f_4, g_4)[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example 5\n",
    "A small example to test that max marginal and Viterbi is not always the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the probability for each possible sequence\n",
      "[ 0.22738372  0.30904651  0.30904651  0.15452326]\n",
      "so the sequences [1,0] and [0,1] are the most probable\n",
      "\n",
      "Viterbi\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Viterbi finds one of them [1,0]\n",
      "\n",
      "exp log_marginal\n",
      "[[ 0.53643023  0.53643023]\n",
      " [ 0.46356977  0.46356977]]\n",
      "According to marginals probability [0,0] should be preferred\n",
      "\n",
      "Samples\n",
      "[[[1 0]\n",
      "  [0 1]]\n",
      "\n",
      " [[0 1]\n",
      "  [0 1]]\n",
      "\n",
      " [[1 0]\n",
      "  [0 1]]\n",
      "\n",
      " [[0 1]\n",
      "  [1 0]]]\n"
     ]
    }
   ],
   "source": [
    "b_5 = 4 # batch_size\n",
    "n_5 = 2 # length of sequence\n",
    "c_5 = 2 # number of classes\n",
    "\n",
    "g_5 = np.zeros((b_5, n_5-1, c_5, c_5)) # independent variables\n",
    "f_5 = np.zeros((b_5, n_5, c_5)) #\n",
    "\n",
    "g_5[:, :, 0, 0] = -np.ones([n_5-1])\n",
    "f_5[:, :, 0] = np.log(2)\n",
    "\n",
    "y_5 = np.zeros([b_5, n_5], dtype=np.int32) # all label belong to class 1\n",
    "y_5[1, :] = [0, 1]\n",
    "y_5[2, :] = [1, 0]\n",
    "y_5[3, :] = [1, 1]\n",
    "\n",
    "y_5_hot = onehot_tensor(y_5, 2)\n",
    "\n",
    "nu_alp_5 = forward_pass(f_5, g_5)\n",
    "nu_bet_5 = backward_pass(f_5, g_5)\n",
    "\n",
    "print(\"the probability for each possible sequence\")\n",
    "print(np.exp(log_likelihood_hot(y_5_hot, f_5, g_5, nu_alp_5, nu_bet_5, mean_batch=False)))\n",
    "print(\"so the sequences [1,0] and [0,1] are the most probable\")\n",
    "print()\n",
    "print(\"Viterbi\")\n",
    "print(viterbi(f_5, g_5))\n",
    "print(\"Viterbi finds one of them [1,0]\")\n",
    "print()\n",
    "print(\"exp log_marginal\")\n",
    "print(np.exp(log_marginal(nu_alp_5, nu_bet_5))[0].T)\n",
    "print(\"According to marginals probability [0,0] should be preferred\")\n",
    "print()\n",
    "print(\"Samples\")\n",
    "print(sampling_ffbs_hot(f_5, g_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6\n",
    "\n",
    "Some short sequences with f and g random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data\n",
      "[[0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "\n",
      "run 0\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.17983768  0.17942531  0.3879971   0.04495993  0.01228512  0.01225695\n",
      "  0.16420977  0.01902813]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "Viterbi\n",
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "exp log_marginal\n",
      "[[ 0.79222002  0.38380506  0.74432967]\n",
      " [ 0.20777998  0.61619494  0.25567033]]\n",
      "\n",
      "run 1\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.14960726  0.17399722  0.0566593   0.15750672  0.11821717  0.13748971\n",
      "  0.05463718  0.15188544]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "Viterbi\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "\n",
      "exp log_marginal\n",
      "[[ 0.53777051  0.57931136  0.37912092]\n",
      " [ 0.46222949  0.42068864  0.62087908]]\n",
      "\n",
      "run 2\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.03555296  0.05750414  0.42338088  0.08451763  0.02621383  0.04239882\n",
      "  0.27544574  0.054986  ]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "Viterbi\n",
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "exp log_marginal\n",
      "[[ 0.60095561  0.16166974  0.7605934 ]\n",
      " [ 0.39904439  0.83833026  0.2394066 ]]\n",
      "\n",
      "run 3\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.01576742  0.26094379  0.02259857  0.09331008  0.01284502  0.21257934\n",
      "  0.07446946  0.30748633]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "Viterbi\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "\n",
      "exp log_marginal\n",
      "[[ 0.39261986  0.50213556  0.12568046]\n",
      " [ 0.60738014  0.49786444  0.87431954]]\n",
      "\n",
      "run 4\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.00652247  0.02564405  0.01751941  0.00858545  0.07549898  0.29683529\n",
      "  0.38213012  0.18726424]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "Viterbi\n",
      "[[1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]]\n",
      "\n",
      "exp log_marginal\n",
      "[[ 0.05827138  0.40450079  0.48167098]\n",
      " [ 0.94172862  0.59549921  0.51832902]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_6 = 8 # batch_size\n",
    "n_6 = 3 # length of sequence\n",
    "c_6 = 2 # number of classes\n",
    "\n",
    "g_6 = np.zeros((b_6, n_6-1, c_6, c_6)) # independent variables\n",
    "f_6 = np.zeros((b_6, n_6, c_6)) #\n",
    "  \n",
    "# define some example data that we will use below to calculate log likelihood\n",
    "y_6 = np.zeros([b_6, n_6], dtype=np.int32) # all label belong to class 1\n",
    "y_6[1, :] = [0, 0, 1]\n",
    "y_6[2, :] = [0, 1, 0]\n",
    "y_6[3, :] = [0, 1, 1]\n",
    "y_6[4, :] = [1, 0, 0]\n",
    "y_6[5, :] = [1, 0, 1]\n",
    "y_6[6, :] = [1, 1, 0]\n",
    "y_6[7, :] = [1, 1, 1]\n",
    "\n",
    "print(\"the data\")\n",
    "print(y_6)\n",
    "print()\n",
    "    \n",
    "y_6_hot = onehot_tensor(y_6, 2)\n",
    "        \n",
    "np.random.seed(42)\n",
    "\n",
    "for i in np.arange(5):\n",
    "    \n",
    "    print('run ' + repr(i))\n",
    "    print()\n",
    "    \n",
    "    f_6[:] = np.random.randn(n_6, c_6) \n",
    "    g_6[:] = np.random.randn(n_6-1, c_6, c_6)  #np.zeros((n_6-1, c_6, c_6)) #np.random.randn(n_6-1, c_6, c_6) \n",
    "    \n",
    "    #print(f_6)\n",
    "    #print(g_6)\n",
    "   \n",
    "    nu_alp_6 = forward_pass(f_6, g_6)\n",
    "    nu_bet_6 = backward_pass(f_6, g_6)\n",
    "\n",
    "    print(\"the probability for each possible sequence\")\n",
    "    print(np.exp(log_likelihood_hot(y_6_hot, f_6, g_6, nu_alp_6, nu_bet_6, mean_batch=False)))\n",
    "    print()\n",
    "    print(\"check that the probability add up to one\")\n",
    "    print(np.sum(np.exp(log_likelihood_hot(y_6_hot, f_6, g_6, nu_alp_6, nu_bet_6, mean_batch=False))))\n",
    "    print()\n",
    "    print(\"Viterbi\")\n",
    "    print(viterbi(f_6, g_6))\n",
    "    print()\n",
    "    print(\"exp log_marginal\")\n",
    "    print(np.exp(log_marginal(nu_alp_6, nu_bet_6))[0].T)\n",
    "    print()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF as latent layer\n",
    "\n",
    "Let us define a latent layer in RNN type model which is a CRF and a likelihood for a sequence $y$ using the following model:\n",
    "\\begin{align}\n",
    "z & \\sim {\\rm CRF}(z|h) \\\\\n",
    "y & \\sim p(y|h,z) = \\prod_i^n p(y_i|h_i,z_i) \\ .\n",
    "\\end{align}\n",
    "This model still has a CRF structure and we can perform exact marginalization. If we denote the normalizer computed with factors $f$ and $g$ by $Z(f,g)$ then we get:\n",
    "\\begin{align}\n",
    "p(y|h) = \\sum_z p(y|h,z) {\\rm CRF}(z|h) = \\frac{Z(f+\\log p(y|z,h),g)}{Z(f,g)} \\ .\n",
    "\\end{align}\n",
    "To compute this likelihood we thus need to perform two forward passes for the graphs defined by $(f,g)$ and $(f+\\log p(y|z,h),g)$.\n",
    "\n",
    "For prediction we note that the graph with both $z$ and $y$ unobserved still defines a tree and exact computation of marginals $p(z_i|h)$ and $p(y_i|h)$ and Viterbi decoding ${\\rm argmax}_{y,z} p(y,z|h)$ are therefore possible. Note however that the more interesting decoding ${\\rm argmax}_{y} p(y|h)$ is not possible because $p(y|h)$ is not a tree. The details will appear later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7\n",
    "\n",
    "Softmax output layer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data\n",
      "[[0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "\n",
      "run 0\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.45186689  0.1355856   0.11416276  0.03217685  0.16506251  0.04482619\n",
      "  0.04506487  0.01125433]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "marginals latent\n",
      "[[ 0.15555653  0.03678695  0.21949861  0.58815792]\n",
      " [ 0.3395483   0.0667786   0.11347221  0.48020089]\n",
      " [ 0.2776246   0.28757036  0.07996116  0.35484387]]\n",
      "\n",
      "marginals out\n",
      "[[ 0.82624296  0.17375704]\n",
      " [ 0.87446722  0.12553278]\n",
      " [ 0.843573    0.156427  ]]\n",
      "\n",
      "run 1\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.40045335  0.20990225  0.10920411  0.08065662  0.10474243  0.05170418\n",
      "  0.02513555  0.01820152]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "marginals latent\n",
      "[[ 0.03788099  0.02809253  0.10356142  0.83046506]\n",
      " [ 0.4577009   0.00694176  0.16712286  0.36823448]\n",
      " [ 0.03763978  0.30786458  0.27705213  0.37744351]]\n",
      "\n",
      "marginals out\n",
      "[[ 0.84131149  0.15868851]\n",
      " [ 0.88444029  0.11555971]\n",
      " [ 0.7314639   0.2685361 ]]\n",
      "\n",
      "run 2\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.3337625   0.14907454  0.20894415  0.09050939  0.08951708  0.03870756\n",
      "  0.06311963  0.02636514]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "marginals latent\n",
      "[[ 0.02269102  0.26869499  0.02649597  0.68211802]\n",
      " [ 0.07342067  0.62589571  0.27656597  0.02411765]\n",
      " [ 0.42825633  0.25310034  0.25811213  0.06053121]]\n",
      "\n",
      "marginals out\n",
      "[[ 0.82171952  0.17828048]\n",
      " [ 0.66330712  0.33669288]\n",
      " [ 0.82332515  0.17667485]]\n",
      "\n",
      "run 3\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.48242984  0.17309034  0.13089746  0.03546433  0.10110311  0.03478242\n",
      "  0.03328449  0.00894802]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "marginals latent\n",
      "[[ 0.0387098   0.12812458  0.01616562  0.81699999]\n",
      " [ 0.48985098  0.36064593  0.04823836  0.10126472]\n",
      " [ 0.14669183  0.56881018  0.03000814  0.25448986]]\n",
      "\n",
      "marginals out\n",
      "[[ 0.84912699  0.15087301]\n",
      " [ 0.87004769  0.12995231]\n",
      " [ 0.78838975  0.21161025]]\n",
      "\n",
      "run 4\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.41314433  0.19737655  0.15958319  0.07247903  0.07752755  0.03719194\n",
      "  0.02936864  0.01332878]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "marginals latent\n",
      "[[ 0.01918642  0.0685266   0.01198585  0.90030114]\n",
      " [ 0.16134426  0.57761962  0.05672652  0.2043096 ]\n",
      " [ 0.06204237  0.66278013  0.11075224  0.16442526]]\n",
      "\n",
      "marginals out\n",
      "[[ 0.85602329  0.14397671]\n",
      " [ 0.78047467  0.21952533]\n",
      " [ 0.72160146  0.27839854]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def log_softmax(z, axis=0):\n",
    "    return z - np.expand_dims(log_sum_exp(z, axis), axis=axis)\n",
    "\n",
    "b_7 = 8 # batch_size\n",
    "n_7 = 3 # length of sequence\n",
    "c_7 = 4 # number of latent classes\n",
    "c_out_7 = 2 # number of classes\n",
    "\n",
    "\n",
    "g_7 = np.zeros((b_7, n_7-1, c_7, c_7)) # independent variables\n",
    "f_7 = np.zeros((b_7, n_7, c_7)) #\n",
    "w_7 = np.random.randn(c_out_7,c_7) # the weights the output with the latent\n",
    "  \n",
    "# define some example data that we will use below to calculate log likelihood\n",
    "y_7 = np.zeros([b_7, n_7], dtype=np.int32) # all label belong to class 1\n",
    "y_7[1, :] = [0, 0, 1]\n",
    "y_7[2, :] = [0, 1, 0]\n",
    "y_7[3, :] = [0, 1, 1]\n",
    "y_7[4, :] = [1, 0, 0]\n",
    "y_7[5, :] = [1, 0, 1]\n",
    "y_7[6, :] = [1, 1, 0]\n",
    "y_7[7, :] = [1, 1, 1]\n",
    "\n",
    "print(\"the data\")\n",
    "print(y_7)\n",
    "print()\n",
    "    \n",
    "y_7_hot = onehot_tensor(y_7, 2)\n",
    "        \n",
    "np.random.seed(42)\n",
    "\n",
    "for i in np.arange(5):\n",
    "    \n",
    "    print('run ' + repr(i))\n",
    "    print()\n",
    "    \n",
    "    f_7[:] = np.random.randn(n_7, c_7) \n",
    "    g_7[:] = np.random.randn(n_7-1, c_7, c_7)  #np.zeros((n_6-1, c_6, c_6)) #np.random.randn(n_6-1, c_6, c_6) \n",
    "    f_out_7 = np.dot(y_7_hot, log_softmax(w_7))\n",
    "    \n",
    "    nu_alp_7 = forward_pass(f_7, g_7) # forward backward to get Z\n",
    "    nu_bet_7 = backward_pass(f_7, g_7)\n",
    "    \n",
    "    nu_out_alp_7 = forward_pass(f_7 + f_out_7, g_7) # forward backward to get Z_out\n",
    "    nu_out_bet_7 = backward_pass(f_7 + f_out_7, g_7)\n",
    "\n",
    "    log_like = logZ(nu_out_alp_7 , nu_out_bet_7) - logZ(nu_alp_7 , nu_bet_7)\n",
    "    \n",
    "    mar = np.exp(log_marginal(nu_out_alp_7, nu_out_bet_7))\n",
    "    mar_out = np.exp(log_softmax( np.dot(mar, np.transpose(w_7)), axis=2))\n",
    "\n",
    "    #print(\"softmax probabilities\")\n",
    "    #print(np.exp(log_softmax(w_7)))\n",
    "    #print()\n",
    "    print(\"the probability for each possible sequence\")\n",
    "    print(np.exp(log_like))\n",
    "    print()\n",
    "    print(\"check that the probability add up to one\")\n",
    "    print(np.sum(np.exp(log_like)))\n",
    "    print()\n",
    "    print(\"marginals latent\")\n",
    "    print(mar[0])\n",
    "    print()\n",
    "    print(\"marginals out\")\n",
    "    print(mar_out[0])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From autoregressive to CRF \n",
    "\n",
    "The causal autoregressive first order Markov model is\n",
    "\\begin{align}\n",
    "p(y|h) = p(y_1|h_1)\\prod_{i=2}^n p(y_i|y_{i-1},h_i) \\ .\n",
    "\\end{align}\n",
    "Each term is given by a softmax\n",
    "\\begin{align}\n",
    "p(y_1|h_1) & = \\frac{\\exp( w_0(y_1,h_1))}{\\sum_{y} \\exp( w_0(y,h_i)) } \\\\\n",
    "p(y_i|y_{i-1},h_i) & = \\frac{\\exp( w(y_{i-1},y_{i},h_{i-1}))}{\\sum_{y} \\exp( w(y_{i-1},y,h_{i-1})) } \\ .\n",
    "\\end{align}\n",
    "We can now compare this with the definition of the CRF and translate to the CRF definition. Defining the indicator\n",
    "function $I({\\rm true})=1$ and $I({\\rm false})=0$ we can write:\n",
    "\\begin{align}\n",
    "f(y_i,h_i) & = I(i=1) w_0(y_1,h_1) - I(i<n) \\log \\sum_{y} \\exp( w(y_i,y,h_i)) \\\\\n",
    "g(y_i,y_{i+1},h_i) & = w(y_{i},y_{i+1},h_{i}) \\ .\n",
    "\\end{align}\n",
    "So the autoregressive model is nothing but a constraint version of the CRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the weight matrix w is laid out like g: b x n-1 x c x c \n",
    "# the weight w0 is: b x c\n",
    "\n",
    "def ar_to_crf(w0, w):\n",
    "    # right now w needs to be 4 dim, could be nice with a version where it could be 2 or 3 dim. \n",
    "    fs = - log_sum_exp(w, np.ndim(w)-2)\n",
    "    fs[:,0] = fs[:,0] + w0\n",
    "    f = np.zeros(tuple(map(sum, zip(np.shape(fs), (0,1,0))))) # add one to length of f\n",
    "    f[:,:-1]=fs \n",
    "    return (f,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0\n",
      "[ 1.  1.  1.  1.]\n",
      "\n",
      "w\n",
      "[[[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]]\n",
      "\n",
      "f\n",
      "[[-0.38629436 -0.38629436 -0.38629436 -0.38629436]\n",
      " [-1.38629436 -1.38629436 -1.38629436 -1.38629436]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "g\n",
      "[[[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Example 7.1\n",
    "\n",
    "w_7_1 = np.zeros(np.shape(g_7))\n",
    "w0_7_1 = np.ones((b_7,c_7))\n",
    "\n",
    "(f_7_1,g_7_1) = ar_to_crf(w0_7_1,w_7_1)\n",
    "\n",
    "print(\"w0\")\n",
    "print(w0_7_1[0])\n",
    "print()\n",
    "print(\"w\")\n",
    "print(w_7_1[0])\n",
    "print()\n",
    "print(\"f\")\n",
    "print(f_7_1[0])\n",
    "print()\n",
    "print(\"g\")\n",
    "print(g_7_1[0])\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not fully observed data\n",
    "\n",
    "We can also handle the situation where the time series is only observed at $n_o\\leq n$ timepoints \n",
    "$(t_1,\\ldots,t_{n_o}$. The model is now\n",
    "\\begin{align}\n",
    "z & \\sim {\\rm CRF}(z|h) \\\\\n",
    "y_o & \\sim p(y_o|h,z) = \\prod_i^{n_o} p(y_{t_i}|h_{t_i},z_{t_i}) \\ .\n",
    "\\end{align}\n",
    "We can perform exact inference as before and infer latent state and marginal for all timepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data - all 4 possible sequences with the first and last symbol observed\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "the data mask - mask out everything but first and last symbol\n",
      "[[1 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "softmax probabilities\n",
      "[[ 1.   0.   0.5]\n",
      " [ 0.   1.   0.5]]\n",
      "\n",
      "the probability for each possible sequence\n",
      "[ 0.41666667  0.08333333  0.08333333  0.41666667]\n",
      "\n",
      "check that the probability add up to one\n",
      "1.0\n",
      "\n",
      "data point 0\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "marginals latent\n",
      "[[ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]\n",
      " [ 0.8  0.   0.2]]\n",
      "\n",
      "marginals out\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "\n",
      "data point 1\n",
      "[0 0 0 0 0 0 0 0 0 1]\n",
      "\n",
      "marginals latent\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "marginals out\n",
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "\n",
      "data point 2\n",
      "[1 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "marginals latent\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "marginals out\n",
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "\n",
      "data point 3\n",
      "[1 0 0 0 0 0 0 0 0 1]\n",
      "\n",
      "marginals latent\n",
      "[[ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]\n",
      " [ 0.   0.8  0.2]]\n",
      "\n",
      "marginals out\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_8 = 4 # batch_size\n",
    "n_8 = 10 # length of sequence\n",
    "c_8 = 3 # number of latent classes\n",
    "c_out_8 = 2 # number of classes\n",
    "\n",
    "g_8 = np.zeros((b_8, n_8-1, c_8, c_8)) # independent variables\n",
    "f_8 = np.zeros((b_8, n_8, c_8)) #\n",
    "  \n",
    "# define some example data that we will use below to calculate log likelihood\n",
    "y_8 = np.zeros([b_8, n_8], dtype=np.int32) # all label belong to class 1\n",
    "y_8[1, :] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "y_8[2, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "y_8[3, :] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "y_mask_8 = np.zeros([b_8, n_8], dtype=np.int32) # we observe first and last timepoint\n",
    "y_mask_8[:] = [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "print(\"the data - all 4 possible sequences with the first and last symbol observed\")\n",
    "print(y_8)\n",
    "print()\n",
    "print(\"the data mask - mask out everything but first and last symbol\")\n",
    "print(y_mask_8)\n",
    "print()\n",
    "    \n",
    "y_8_hot = onehot_tensor(y_8, 2)\n",
    "        \n",
    "persist = 1000\n",
    "g_8[:] = (persist+1)*np.eye(c_8,c_8) - persist*np.ones((c_8,c_8)) # np.random.randn(n_7-1, c_7, c_7)\n",
    "    \n",
    "w_8 = (persist+1)*np.eye(c_out_8,c_8) - persist*np.ones((c_out_8,c_8)) #np.zeros((c_out_8,c_8)) # the weights the output with the latent\n",
    "\n",
    "f_out_8 = np.dot(y_8_hot*np.expand_dims(y_mask_8, 2), log_softmax(w_8))\n",
    "    \n",
    "nu_alp_8 = forward_pass(f_8, g_8) # forward backward to get Z\n",
    "nu_bet_8 = backward_pass(f_8, g_8)\n",
    "    \n",
    "nu_out_alp_8 = forward_pass(f_8 + f_out_8, g_8) # forward backward to get Z_out\n",
    "nu_out_bet_8 = backward_pass(f_8 + f_out_8, g_8)\n",
    "\n",
    "log_like = logZ(nu_out_alp_8 , nu_out_bet_8) - logZ(nu_alp_8 , nu_bet_8)\n",
    "    \n",
    "mar = np.exp(log_marginal(nu_out_alp_8, nu_out_bet_8))\n",
    "mar_out = np.exp(log_softmax( np.dot(mar, np.transpose(w_8)), axis=2))\n",
    "\n",
    "print(\"softmax probabilities\")\n",
    "print(np.exp(log_softmax(w_8)))\n",
    "print()\n",
    "print(\"the probability for each possible sequence\")\n",
    "print(np.exp(log_like))\n",
    "print()\n",
    "print(\"check that the probability add up to one\")\n",
    "print(np.sum(np.exp(log_like)))\n",
    "print()\n",
    "\n",
    "for i in np.arange(b_8):\n",
    "    \n",
    "    print('data point ' + repr(i))\n",
    "    print(y_8[i])\n",
    "    print()\n",
    "    print(\"marginals latent\")\n",
    "    print(mar[i])\n",
    "    print()\n",
    "    print(\"marginals out\")\n",
    "    print(mar_out[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF decoder with attention\n",
    "\n",
    "Consider a (bi-directional) encoder that gives a sequence $h$ that feeds into (bi-directional) decoder that gives a sequence $s$.  \n",
    "One way of implementing a CRF decoder with attention to $h$ is to let the CRF\n",
    "factors $f$ and $g$ be functions of context vectors $c_i$ and $\\hat{c}_i$ the and decoder state $s_i$:\n",
    "$f(y_i,c_i,s_i)$ and $g(y_i,y_{i+1},\\hat{c}_i,s_i)$. We can also let the context vectors depend on the labels: $c_i(y_i)$ and $\\hat{c}_i(y_i,y_{i+1})$. \n",
    "\n",
    "An important difference to the usual language model is that the decoder cannot use the already emitted symbols because then the RNN would act as an label information channel violating the nearest neighbour only requirement of the CRF. This restriction makes the decoder a much less powerful language model. This will instead be the task of the CRF. The biggest benefit of the CRF is that we can make exact inference.\n",
    "\n",
    "These are the equations for calculating $f(y_i,c_i,s_i)$. The calculation of $g(y_i,y_{i+1},\\hat{c}_i,s_i)$ is completely analogeous and can be found in the code. Step one is to calculate attention weights for each label, decoder position and encoder position tuple:    \n",
    "$$\\alpha_{ij}(y_i) = \\alpha_{ij}(y_i,s_i,h)= \\frac{\\exp(e(y_i,s_i,h_j))}{\\sum_{j'}\\exp(e(y_i,s_i,h_{j'}))}\n",
    "\\ . \n",
    "$$\n",
    "In the second step we calculate the context vector for each label and decoder position pair:\n",
    "\\begin{align*}\n",
    "c_i(y_i) & = \\sum_{j=1}^{n_{\\rm in}} \\alpha_{ij}(y_i) \\, h_j \\ .\n",
    "\\end{align*}\n",
    "Then finally we compute $f(y_i,c_i,s_i)$. This is implemented as a simple linear function in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2, 20, 10)\n",
      "(8, 2, 20, 15)\n",
      "(8, 20, 2)\n",
      "(8, 2, 2, 19, 10)\n",
      "(8, 2, 2, 19, 15)\n",
      "(8, 19, 2, 2)\n",
      "\n",
      "log Z\n",
      "[ 95.0911613   84.36973226  60.13453541  70.4113463   84.07281802\n",
      "  78.76179297  56.69576373  98.13610992]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 9 - instead of implementing the actual encode-decoder architecture the example just assumes that \n",
    "# encoder states h, decoder states s and the attention \"error\" e is given. Assumes a simple linear f(y_i,c_i,s_i)  \n",
    "b_9 = 8\n",
    "c_9 = 2\n",
    "n_9 = 20\n",
    "n_in = 10\n",
    "h_dim = 15\n",
    "s_dim = 7\n",
    "\n",
    "e_f = np.random.randn(b_9, c_9, n_9, n_in); # error term in attention softmax for f - b x c x n x n_in\n",
    "e_g = np.random.randn(b_9, c_9, c_9, n_9-1, n_in); # error term in attention softmax for g - b x c x c x n-1 x n_in\n",
    "h = np.random.randn(b_9, n_in, h_dim) # output from encoder - b x n_in x h_dim\n",
    "s = np.random.randn(b_9, n_9, s_dim) # output from decoder - b x n x s_dim\n",
    "w_fc = np.random.randn(c_9, h_dim) # weights for context verctor - c x h_dim\n",
    "w_fs = np.random.randn(c_9, s_dim) # weights for decoder - c x s_dim\n",
    "w_gc = np.random.randn(c_9, c_9, h_dim) # weights for context verctor - c x c x h_dim\n",
    "w_gs = np.random.randn(c_9, c_9, s_dim) # weights for decoder - c x c x s_dim\n",
    "\n",
    "alpha_f = np.exp(log_softmax(e_f, axis=3)) # b x c x n x n_in \n",
    "ctxt_f = np.sum(np.expand_dims(alpha_f, 4) * np.reshape(h, (b_9, 1, 1, n_in, h_dim)), axis=3) # b x c x n x h_dim\n",
    "f_9 = np.swapaxes(np.sum( ctxt_f * np.reshape(w_fc, (1, c_9, 1, h_dim)), axis=3), 1, 2) + np.dot(s, np.transpose(w_fs)) # b x n x c\n",
    "\n",
    "print(np.shape(alpha_f))\n",
    "print(np.shape(ctxt_f))\n",
    "print(np.shape(f_9))\n",
    "\n",
    "alpha_g = np.exp(log_softmax(e_g, axis=4)) # b x c x c x n-1 x n_in\n",
    "ctxt_g = np.sum(np.expand_dims(alpha_g, 5) * np.reshape(h, (b_9, 1, 1, 1, n_in, h_dim)), axis=4) # b x c x c x n-1 x h_dim\n",
    "g_9 = np.transpose(np.sum( ctxt_g * np.reshape(w_gc, (1, c_9, c_9, 1, h_dim)), axis=4),[0, 3, 1, 2]) + np.tensordot(s[:,:-1], w_gs, ([2, 2]))\n",
    "\n",
    "print(np.shape(alpha_g))\n",
    "print(np.shape(ctxt_g))\n",
    "print(np.shape(g_9))\n",
    "\n",
    "nu_alp_9 = forward_pass(f_9, g_9) # forward backward to get Z\n",
    "nu_bet_9 = backward_pass(f_9, g_9)\n",
    " \n",
    "print()\n",
    "print(\"log Z\")\n",
    "print(logZ(nu_alp_9, nu_bet_9))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
